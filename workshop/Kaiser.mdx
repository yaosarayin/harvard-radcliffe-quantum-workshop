---
title: Tackling Loopholes in Experimental Tests of Bell’s Inequality
---

David Kaiser (MIT)

<div class="videoWrapper">

<iframe
  width="100%"
  height="500px"
  src="https://www.youtube.com/embed/5z0w4O1YvG8"
  frameborder="0"
  allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>
</div>

<br />
<br />

DAVID KAISER: Great. Well, thank you very much. Thank you both to Jacob and to Ned for organizing this and bringing us together. It's been— I’ve already been enjoying it very much, and I very much look forward to our continuing discussions.

So today, in these brief remarks, I want to talk about some of the recent work in experimental tests of Bell’s Inequality, something quite recent. So I’m going to start with a kind of lightning reminder about Bell’s Inequality, which I’m sure is familiar to all or certainly most of us, and I’ll pivot to some of the more recent work to both identify or elucidate a series of what are now called “loopholes” in these experimental tests, and then look in particular at my own group's recent work in what we've called “Cosmic Bell tests.”

This will go by pretty quickly. There's a review article on this material that's available on arXiv. Especially for the last part when I talk about my own group's work, that's based on some work from colleagues along the way, and again, I’d of course be delighted to talk more about all this when we have time.

So as we know, John Bell introduced what we will now often call Bell’s Inequality, at least the first version, late in 1964. And just to set nomenclature, I want to clarify I’m going to be talking about two particle tests, not multi-particle GHZ states or other fun stuff. For today, I’ll focus on tests involving pairs of entangled particles, and in this cartoon version of a Bell test, we have particles emitted from a source, they travel toward one of two identical detectors. At each detector we have a choice of what measurement to perform, so I’ll call that detector settings with lowercase letters ”a” and “b,” choosing, say, the measurement basis, and then the detectors yield a measurement outcome, and I’ll label those with capital letters. Spin Up or Spin Down, and so on. Of course, we can then be a bit more sophisticated and put together correlation functions comparing the measurement outcomes at the two detectors, especially as we choose different detector settings.

And then the real magic comes in when we consider certain combinations of those correlations, and here's one example that is familiar, I’m sure, for you, first introduced in this form by Clauser, Horne, Shimony, and Holt, but following very closely on Bell's original reasoning.

And the upshot of these tests, as Bell argued, was that **any effort to account for these measurement outcomes in a framework that obeys what he would eventually call “local causality,” such that the measurement outcome at one device is independent of both the decision of what measurement to perform and the measurement outcome at the distant device — any such framework should have an upper bound, a limit on how strongly correlated the outcomes of all these measurements can be**. And this is a version of the famous Bell Inequality, and of course as we all know, and as Bell reminded us even back in 1964, quantum theory predicts there should be situations where this bound is violated. Certain quantum systems should reveal stronger correlations than this locally causal or Bell bound would allow.

This has been experimentally tested now for very nearly fifty years. The first published experimental test was published in 1972, we’re coming up close to the anniversary. One of my favorite recent experimental tests, one of the most ambitious and perhaps audacious, was conducted by [Professor Jian-Wei Pan](https://quantum.ustc.edu.cn/web/en/node/32) and his very large group at the Chinese Academy of Sciences. They built and launched a purpose-built satellite into low Earth orbit — it's up orbiting now — beaming down pairs of polarization-entangled photons to the surface of the earth, and with that setup, they were able to conduct a Bell test where the two detector stations were 1200 kilometers apart. This is pretty awesome.

Of course, they launched that satellite not only to conduct Bell tests, as fun as those are, but actually as a critical piece of what is now a transcontinental quantum encryption communication network. And so working together with Jian-Wei Pan’s own former mentor Anton Zeilinger and his group in Austria, they conducted a really quite remarkable quantum encrypted video chat with frequently updated encryption keys.

And the point, as I’m sure many of you know, is that the quantum key distribution or quantum encryption protocols will only be safe against certain kinds of eavesdropping attacks if those photons are governed by entanglement rather than by any locally causal description. If local causality is still viable, then at least in principle these communication channels could be vulnerable to all kinds of eavesdropping hacks. So many encryption protocols now embed Bell tests to try to demonstrate or verify the security of a quantum channel, so in some sense, the stakes for conducting Bell tests only seem higher now than even back in John Bell's day.

So the standard interpretation of now fifty years of these experimental tests is that local causality simply is incompatible with experiment, but again as I’m sure many of you know, that conclusion itself relies on several assumptions, and that's what opens up the space for these so-called loopholes. Let me talk briefly about some of the better known ones. The first loophole that was identified, and indeed the first one addressed experimentally, is now often called the **“locality loophole.”** If we're not very careful with the space-time arrangement of the relevant events in our experiment, then it'll be very straightforward to give a locally causal explanation of measured violations of Bell’s Inequality. If we first select the detector settings and then later emit the entangled particles, conduct one measurement then wait a while before the second one, of course, it’s not hard to imagine perfectly locally causal information flows that could account for that. Alice could simply tweet at Bob — I’d like to think that she'd use the hashtag **#keepingitlocal**.

So how do we avoid that? Of course, we have to be very careful with a space-like separation of appropriate events so that this explanation would not be available to give a locally causal account of the measurement outcomes.

This is what drove Alain Aspect’s very famous and beautiful experiments in the early 1980s. This loophole was really closed, I think more conclusively, some years later by Zeilinger's group, at that point in Innsbruck in Austria. There's a second loophole quite conceptually distinct from that locality one.

The second one is often called the “fair sampling” or “detector efficiency” loophole and this is asking about whether the set of measurements that were completed were on a statistically representative sample of all the particles that were at play.

So the idea is that any real measurement device does not perform measurements on every single particle that it encounters. There's some limited efficiency. And so the argument is that if the devices miss too many particles, then it would be consistent with local causality to measure violations of Bell’s Inequality, even if the actual full population of these particles was governed by local causality. So to rule out such kind of statistical unrepresentativeness arguments, as was found quite early in the game, you need actually pretty high detector efficiencies. You have to be able to get to perform joint measurements on roughly 80% of the particles, and you can finesse that, but it's a technological challenge rather than a kind of conceptual one. That was identified as early as 1970 in the literature and it took 30 years before the first experiments could actually even try to address that because it was such a steep technological challenge. And in all of these experiments that I’m listing here, each of these aimed only to close this fair sampling loophole and not even to try to address the locality loophole and vice versa. So as recently as 2013 and 2014, we had a series of experiments that had closed one loophole but not the other.

Again, as many of you may know, and really I think a very substantial milestone in this in this work, came as recently as just five years ago. It was a group led by Ronald Hanson, shown here at TU Delft, where they managed to [close both the locality and the fair sampling loopholes in the same experiment](https://www.nature.com/articles/nature15759). This became headline news in [The New York Times](https://www.nytimes.com/2015/10/22/science/quantum-theory-experiment-said-to-prove-spooky-interactions.html). Just weeks later, remarkably close on their heels, two other groups also managed to complete Bell tests that closed both the locality and the fair sampling loopholes at the same time, [one group](https://arxiv.org/abs/1511.03190#:~:text=Significant%2Dloophole%2Dfree%20test%20of%20Bell's%20theorem%20with%20entangled%20photons,-Marissa%20Giustina%2C%20Marijn&text=Local%20realism%20is%20the%20worldview,than%20the%20speed%20of%20light) again led by Anton Zeilinger in Vienna, [the other](https://arxiv.org/abs/1511.03189) led by Krister Shalm at NIST in Boulder.

Well, it turns out that Bell tests are most vulnerable — in a way I’ll cash out in a moment — they're most susceptible to a third loophole, still conceptually distinct from those first two. This one's often called the **“freedom of choice” loophole**; sometimes you'll see it referred to as **“measurement dependence”** or **“settings dependence”** loopholes. The question here is whether the detector settings, the choice of measurement bases, had any correlation with the properties of the particles that were emitted. And so this could take the form, for example, of a kind of common cause argument. So if we go back again to a cartoon version of a Bell test, we imagine some random process is used to determine the selection of detector settings at each side, and so if something in the shared causal past had either affected that process or merely gotten a preview of it and shared that with the source of entangled particles, then that could produce a correlation, an unintended, perhaps, correlation, between the properties of the particles that are emitted and the selection of measurements to be performed. If you want to be a little more formal about that, what this means is when we construct these conditional probabilities, we have to make sure we respect what's called the **“law of total probability,”** which is actually kind of textbook example from elementary probability in statistics. We have to kind of propagate through this potential dependence — it's the kind of moral equivalent of the chain rule from ordinary calculus.

In Bell's original derivations, he actually had overlooked this. He had tacitly assumed that there was a complete independence between the properties of those particles and the selection of measurements to be performed. From Bayes’ theorem, we can turn that around and say that's the same as saying the selection of measurement bases is thoroughly independent from the properties of the particles. When this was pointed out to Bell a dozen years later, he said he agreed. He said in his earlier work he'd assumed that the settings of the instruments are in some sense free variables, say at the whim of the experimenters, or in any case not determined in the overlap of the backward light cones. There's been a lot of work on this topic recently over the last decade or so. I will refer you to this recent paper that I wrote with some colleagues. It has extensive references, we're certainly not the first on this at all.

But if we if we don't assume by fiat that there couldn't possibly be any coordination or correlation, then it turns out one can build explicitly locally causal models that are remarkably efficient as measured by, for example, the mutual information. In fact, they're more than 20 times more efficient at mimicking all the predictions from ordinary quantum mechanics as, say, communication or signaling models that would exploit, say, the first loophole. So you can — again there's a lot of work on this now — but you can actually build locally causal models that exploit this third loophole. I just want to pause and say look at how little correlation that requires. This does not imply that the physicists would be either robots or zombies, although some of us might appear that way. It’s not the same as superdeterminism suggesting that every single choice of ours was predetermined at some initial Cauchy surface. It's an incredibly small amount of correlation that would be required. So if that's the case for these Bell tests, and my colleagues and I wondered, how might we try to either address or perhaps even instantiate a kind of independence between the properties of the entangled particles and the measurements to be performed? How might we do that experimentally?

JACOB BARANDES: Just a note, David, you've got two minutes.

DAVID KAISER: Thanks Jacob, wrapping up. Our choice was to turn the universe itself into a pair of random number generators. So I’m going to go quickly here, here's a conformal diagram showing the entire history of the observable universe with a few events highlighted. Sometime early in cosmic history a quasar on one side of the universe emitted some light that began making its way toward the earth, some time later a quasar on the other side of the universe emitted its light and it made its way, began traveling toward the earth. Before those astronomical photons reached our telescopes, while they're en route, here on earth we would emit our own pair of entangled particles. While those particles were in flight, we then performed very rapid measurements on the astronomical photons using the outcomes of those measurements to then set the detector settings. Let me skip over this part in the interest of time, though I’d be delighted to talk about it.

This interested Anton Zeilinger, much to my eternal joy, and so with Andy Friedman, Jason Gallicchio, eventually Alan Guth and others, we built this new team. So that's what brought us to the top of this absolutely drop-dead gorgeous mountaintop observatory on the island of La Palma in the Canary Islands just three years ago.

Here's a photograph of our setup. We had three main stations. We produced polarization-entangled photons using what is now the kind of industry standard way, spontaneous parametric down conversion, this is Anton's group and their wizardry. And then just to make a long story short, we transmitted those through open air, free space transmission across the island. We were able to close a locality loophole but not that second loophole, not the fair sampling one. We actually measured only a tiny fraction of all the pairs that were emitted. However, what we could do is try to address that third loophole. So waiting at those detector stations were two, again, absolutely gorgeous instruments, these four-meter telescopes, one on each side, with which we could collect light from very, very distant astronomical objects. So while the entangled pair was in flight on the island, so before it reaches the detector station, we perform very rapid, sort of microsecond windows, measurements of light from assigned quasars on opposite sides of the sky. If within that one microsecond window the light from the astronomical source happened to be more blue than some reference wavelength, then we would measure the entangled photon in one basis, if during that very brief window the astronomical photons were more red than the reference wavelength, then we would rotate the bases for our earthbound measurement. We did this with about 20,000 pairs of entangled photons, and when the dust settled, it will surprise no one to hear that we found very robust violations of Bell’s Inequality by actually more than nine standard deviations, while actually pushing back or constraining this locally causal possible explanation into deep into cosmic history. So, any of these locally causal scenarios is now limited to within the past light cones of either of those quasar emission events, the more recent of which is about eight billion years ago. You can quantify that further. So we excluded these locally causal mechanisms, these third loophole mechanisms, from 96% of the four-volume of the past light cone of the experiment reckoned from the Big Bang forward. Pretty proud of that.

So here, I’m going to wrap up; Jacob, almost done. To account for these results, and indeed the constellation that had come before, we could finally agree to just reject all locally causal alternatives to quantum mechanics, noting that no experiment had addressed two or more loopholes until 2015. It's pretty recent that this is kind of, we might say, forced on us experimentally. Or, if one really wants to retain local causality, now we have much less conceptual terrain to work with. We could appeal to loophole 2, which our group did not even try to address, but then make some argument as to why other experiments had to appeal to loophole 3. We could assume that nature really does exploit these freedom of choice loopholes, and then the correlations were engineered at an appropriately early time, perhaps even during primordial inflation, when all the past light cones for all these events are comfortably overlapped. We could appeal to a kind of initial conditions, a kind of Cauchy surface argument, which is similar, I think, to what a Gerard ‘t Hooft argues, and actually argue about superdeterminism. Or some authors now appeal to retrocausality: maybe the selection of detector settings is actually influenced from the future backwards. But that's sort of what's left. So I want to share with you: these cosmic Bell experiments leverage genuinely astrophysical scales to test the foundations of quantum mechanics, they now tightly constrain locally causal alternatives a way that was not available to us until quite recently, and they are a lot of fun to work on. Thank you very much; I look forward to our discussion.
